{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObp/M4A9uL6/HqJKMq+NdY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamg-upv/0Reto21d_ago24_LLMclassification/blob/main/py/Reto21dias_24_preprocesamiento_de_datos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wybTcY5oZnaA",
        "outputId": "a87f2e08-f7e8-4fad-e391-274d2492f297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '0Reto21d_ago24_LLMclassification'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 56 (delta 23), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (56/56), 1.55 MiB | 3.13 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n"
          ]
        }
      ],
      "source": [
        "#conectar con repositorio github publico para acceso a los datasets\n",
        "!git clone https://github.com/jamg-upv/0Reto21d_ago24_LLMclassification.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#conecar a google drive para guardar alli los outputs que quiera mantener (y que no se me olvide descargarlos) luego los subiré a Git para su uso posterior\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mlMCVq3yZw5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Leer el archivo CSV (es un CSV de Open Office si lo genero con MSexcel no se mapea bien sin dar atributos adicionales)\n",
        "hiwp_data = pd.read_csv(\"/content/0Reto21d_ago24_LLMclassification/datasets/hiwp_componentsv2.csv\")\n",
        "\n",
        "# Seleccionar las filas de la 1 a la 12 (índices 0 a 11) que tienen las definiciones de componentes de HIWP. Comentar si quiero trabajar con filas adicionales\n",
        "# limito explicitamente a las dos primeras columnas que son los datos con los que quiero trabajar (la tercera tiene comentarios de clasificacion de las filas articulos)\n",
        "hiwp_data = hiwp_data.iloc[:, 0:2]\n",
        "\n",
        "# Mostrar el DataFrame resultante\n",
        "print(hiwp_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xccVDKgIbZOw",
        "outputId": "8a81a6f5-543f-4922-ba5b-b7e019cef811"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Component                                        Description\n",
            "0   Select&Recruitment  Rigorous Selection and Recruitment Selecting t...\n",
            "1             Training  Continuous Training and Development Investing ...\n",
            "2          Empowerment  Employee participation Autonomy and Empowermen...\n",
            "3          PerformEval  Performance Evaluation and Feedback Providing ...\n",
            "4        Compensations  Competitive Compensation and Benefits Offering...\n",
            "5              FairJob  Fair Work Practices Treating employees fairly ...\n",
            "6        Communication  Open and Transparent Communication Sharing inf...\n",
            "7               Career  Career and Succession Planning Helping employe...\n",
            "8        LeaderDevelop  Leadership Development A conscious effort to d...\n",
            "9      WorkLifeBalance  Work-Life Balance Implementing policies and pr...\n",
            "10         Recognition  Performance Recognition Recognizing and reward...\n",
            "11     PositiveCulture  Positive Organizational Culture Fostering an o...\n",
            "12     LongDescripHIWP  High Involvement work practices High commitmen...\n",
            "13          RemoteWork  Remote work refers to a flexible work arrangem...\n",
            "14             GrenHRM  Green Human Resource Management (GHRM) refers ...\n",
            "15    PeopleAnaliytics  Human Resource Analytics refers to the systema...\n",
            "16         NonTradWork  Non-traditional work arrangements refer to emp...\n",
            "17           Hauff2022  High-performance work practices, employee well...\n",
            "18            Song2021  High involvement work systems and organization...\n",
            "19          Marin 2016  Deconstructing AMO framework: a systematic rev...\n",
            "20           Ahmad2022  The impact of green HRM on green creativity: m...\n",
            "21        Houeland2022  Not my task': Role perceptions in a green tran...\n",
            "22          Becker2022  Surviving remotely: How job control and loneli...\n",
            "23         Burbano2022  Mitigating Gig and Remote Worker Misconduct: E...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Promtp en claude3.5"
      ],
      "metadata": {
        "id": "5oTsq_MBZ1j0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tengo un dataframe de nombre \"\"hiwp_data\"\" donde la primera columna es una etiqueta del texto y la segunda columan es el texto que quiero procesar con embeedings mas adelante. Pero primero quiero hacer un preprocesamiento del texto para limpiarlo y dejarlo mejor preparado para las tareas posteriores (calcula la distancia de cada fragmento de texto sobre otros fragmentos de texto). Quiero poder hacer una o varias de estas tareas \"\"\"1. Lowers all characters\n",
        "2. Removes all extra white spaces if there is one\n",
        "3. Removes all special characters including numbers and punctuations\n",
        "4. Stopword Removal. Eliminating common, non-content words that don't carry much meaning and can clutter the data. You would like to extend this list.\n",
        "5. Removing words less then 3 characters. You may want to keep some key acronyms before applying this step\n",
        "6. Stemming/Lemmatization. Reducing words to their base form (stem) or dictionary form (lemma)\n",
        "7. Bigrams and Trigrams. Building a sequence of two (bigrams) or three (trigrams) consecutive words. You can also make your own bigrams and trigrams manually.\n",
        "8. Tokenisation. Breaks down text data into smaller, more manageable units  called tokens.\"\"\" ¿en qué orden me aconsejas realizarlas? ¿puedes prepararme fragmentos de codigo que realicen cada una de esas operaciones en python apra correr en google colab?"
      ],
      "metadata": {
        "id": "Wja86uZXZy2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Te aconsejaría realizar las operaciones de preprocesamiento en el siguiente orden:\n",
        "\n",
        "Convertir a minúsculas\n",
        "Eliminar espacios en blanco extra\n",
        "Eliminar caracteres especiales, números y puntuación\n",
        "Tokenización\n",
        "Eliminar stopwords\n",
        "Eliminar palabras de menos de 3 caracteres\n",
        "Lematización (o stemming)\n",
        "Creación de bigramas y trigramas\n",
        "Este orden permite una limpieza progresiva del texto, comenzando con operaciones más generales y avanzando hacia las más específicas. La tokenización se realiza antes de eliminar stopwords y palabras cortas para facilitar estas operaciones. La lematización se hace después de eliminar palabras cortas para no procesar innecesariamente tokens que luego serán eliminados. Finalmente, se crean los bigramas y trigramas con el texto ya limpio y procesado."
      ],
      "metadata": {
        "id": "fRGzJHFtackw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Instalar las dependencias necesarias\n",
        "import pkg_resources\n",
        "import subprocess\n",
        "\n",
        "required_packages = [\n",
        "    'nltk ',\n",
        "    'pandas'\n",
        "]\n",
        "\n",
        "installed_packages = {pkg.key for pkg in pkg_resources.working_set}\n",
        "\n",
        "for package in required_packages:\n",
        "    if package in installed_packages:\n",
        "        print(f\"{package} ya está instalado.\")\n",
        "    else:\n",
        "        print(f\"{package} no está instalado. Instalando...\")\n",
        "        subprocess.check_call([\"pip\", \"install\", package])\n",
        "        print(f\"{package} ha sido instalado.\")\n",
        "\n",
        "print(\"\\nTodas las bibliotecas necesarias están ahora instaladas.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BahBJup_avFf",
        "outputId": "82b3a8a4-b0d1-4b76-e1c3-358bf88393e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk  no está instalado. Instalando...\n",
            "nltk  ha sido instalado.\n",
            "pandas ya está instalado.\n",
            "\n",
            "Todas las bibliotecas necesarias están ahora instaladas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# Descargar recursos necesarios de NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Cargar el DataFrame (asumiendo que ya está cargado como hiwp_data)\n",
        "# hiwp_data = pd.read_csv('tu_archivo.csv')  # Descomenta esta línea si necesitas cargar el archivo\n",
        "\n",
        "# Función para preprocesar el texto\n",
        "def preprocess_text(text):\n",
        "    # 1. Convertir a minúsculas\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Eliminar espacios en blanco extra\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # 3. Eliminar caracteres especiales, números y puntuación\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # 4. Tokenización\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 5. Eliminar stopwords\n",
        "    stop_words = set(stopwords.words('english'))  # Cambia 'spanish' por 'english' si el texto está en inglés\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # 6. Eliminar palabras de menos de 3 caracteres\n",
        "    tokens = [token for token in tokens if len(token) >= 3]\n",
        "\n",
        "    # 7. Lematización\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # 8. Crear bigramas y trigramas\n",
        "    bigrams = list(ngrams(tokens, 2))\n",
        "    trigrams = list(ngrams(tokens, 3))\n",
        "\n",
        "    # Unir tokens, bigramas y trigramas\n",
        "    processed_text = ' '.join(tokens + [f'{b[0]}_{b[1]}' for b in bigrams] + [f'{t[0]}_{t[1]}_{t[2]}' for t in trigrams])\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Aplicar el preprocesamiento a la columna de texto\n",
        "hiwp_data['processed_text'] = hiwp_data['Description'].apply(preprocess_text)\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame procesado\n",
        "print(hiwp_data.head())"
      ],
      "metadata": {
        "id": "vijftp8NZz4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}